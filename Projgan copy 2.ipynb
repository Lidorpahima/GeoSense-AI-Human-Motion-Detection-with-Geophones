{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting data loading...\n",
      "Total data points loaded: 3367075\n",
      "Created 3366 sequences with length 1000.\n",
      "Reverted Classifier DataLoaders to standard TensorDataset (no augmentation).\n",
      "Loading pre-trained Autoencoder from best_dilated_ae_A.pth\n",
      "\n",
      "--- Part 5: Classifier with Transformer Head & Custom Loss ---\n",
      "Loaded best dilated encoder from best_dilated_encoder_A.pth for classifier.\n",
      "\n",
      "Starting Classifier training (Transformer Head, Custom Loss, Augmentation)...\n",
      "\n",
      "--- Fine-tuning Phase 1 (Custom Loss & Aug): Training Classifier Head Only ---\n",
      "CLF P1 Custom Epoch 1/200 - 0.97s - Train Acc: 0.2530 - Val Acc: 0.3010 - LR: 1.0e-03\n",
      "  New best P1 Custom model saved with val_acc: 0.3010\n",
      "CLF P1 Custom Epoch 2/200 - 1.10s - Train Acc: 0.2810 - Val Acc: 0.3525 - LR: 1.0e-03\n",
      "  New best P1 Custom model saved with val_acc: 0.3525\n",
      "CLF P1 Custom Epoch 3/200 - 0.78s - Train Acc: 0.2810 - Val Acc: 0.2653 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 4/200 - 0.83s - Train Acc: 0.3001 - Val Acc: 0.2495 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 5/200 - 0.78s - Train Acc: 0.2950 - Val Acc: 0.2495 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 6/200 - 0.76s - Train Acc: 0.3128 - Val Acc: 0.5743 - LR: 1.0e-03\n",
      "  New best P1 Custom model saved with val_acc: 0.5743\n",
      "CLF P1 Custom Epoch 7/200 - 0.71s - Train Acc: 0.3137 - Val Acc: 0.3584 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 8/200 - 0.69s - Train Acc: 0.3281 - Val Acc: 0.3822 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 9/200 - 0.98s - Train Acc: 0.3120 - Val Acc: 0.3663 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 10/200 - 0.70s - Train Acc: 0.3294 - Val Acc: 0.3010 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 11/200 - 0.71s - Train Acc: 0.3298 - Val Acc: 0.3505 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 12/200 - 0.74s - Train Acc: 0.3272 - Val Acc: 0.3584 - LR: 1.0e-03\n",
      "CLF P1 Custom Epoch 13/200 - 0.69s - Train Acc: 0.3383 - Val Acc: 0.3545 - LR: 2.0e-04\n",
      "CLF P1 Custom Epoch 14/200 - 0.68s - Train Acc: 0.3455 - Val Acc: 0.3584 - LR: 2.0e-04\n",
      "CLF P1 Custom Epoch 15/200 - 0.70s - Train Acc: 0.3400 - Val Acc: 0.3604 - LR: 2.0e-04\n",
      "CLF P1 Custom Epoch 16/200 - 0.98s - Train Acc: 0.3379 - Val Acc: 0.3802 - LR: 2.0e-04\n",
      "CLF P1 Custom Epoch 17/200 - 0.68s - Train Acc: 0.3413 - Val Acc: 0.3624 - LR: 2.0e-04\n",
      "CLF P1 Custom Epoch 18/200 - 0.70s - Train Acc: 0.3497 - Val Acc: 0.3683 - LR: 2.0e-04\n",
      "CLF P1 Custom Epoch 19/200 - 0.73s - Train Acc: 0.3430 - Val Acc: 0.3604 - LR: 4.0e-05\n",
      "CLF P1 Custom Epoch 20/200 - 0.66s - Train Acc: 0.3455 - Val Acc: 0.3624 - LR: 4.0e-05\n",
      "CLF P1 Custom Epoch 21/200 - 0.68s - Train Acc: 0.3442 - Val Acc: 0.3624 - LR: 4.0e-05\n",
      "CLF P1 Custom Epoch 22/200 - 0.81s - Train Acc: 0.3459 - Val Acc: 0.3564 - LR: 4.0e-05\n",
      "CLF P1 Custom Epoch 23/200 - 0.86s - Train Acc: 0.3472 - Val Acc: 0.3644 - LR: 4.0e-05\n",
      "CLF P1 Custom Epoch 24/200 - 0.68s - Train Acc: 0.3425 - Val Acc: 0.3663 - LR: 4.0e-05\n",
      "CLF P1 Custom Epoch 25/200 - 0.70s - Train Acc: 0.3434 - Val Acc: 0.3624 - LR: 8.0e-06\n",
      "CLF P1 Custom Epoch 26/200 - 0.77s - Train Acc: 0.3468 - Val Acc: 0.3624 - LR: 8.0e-06\n",
      "Classifier P1 Custom Early stopping at epoch 26\n",
      "Classifier Phase 1 Custom training finished.\n",
      "\n",
      "--- Fine-tuning Phase 2 (Custom Loss & Aug): Unfreezing Last 2 Encoder Conv Blocks ---\n",
      "CLF P2 Custom Epoch 1/200 - 1.06s - Train Acc: 0.3154 - Val Acc: 0.6000 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.6000\n",
      "CLF P2 Custom Epoch 2/200 - 1.46s - Train Acc: 0.3281 - Val Acc: 0.4079 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 3/200 - 1.09s - Train Acc: 0.3417 - Val Acc: 0.6812 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.6812\n",
      "CLF P2 Custom Epoch 4/200 - 1.15s - Train Acc: 0.3646 - Val Acc: 0.7089 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.7089\n",
      "CLF P2 Custom Epoch 5/200 - 1.06s - Train Acc: 0.3680 - Val Acc: 0.7386 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.7386\n",
      "CLF P2 Custom Epoch 6/200 - 1.28s - Train Acc: 0.4385 - Val Acc: 0.7366 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 7/200 - 1.25s - Train Acc: 0.4185 - Val Acc: 0.7683 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.7683\n",
      "CLF P2 Custom Epoch 8/200 - 1.15s - Train Acc: 0.4597 - Val Acc: 0.7782 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.7782\n",
      "CLF P2 Custom Epoch 9/200 - 1.19s - Train Acc: 0.5157 - Val Acc: 0.7842 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.7842\n",
      "CLF P2 Custom Epoch 10/200 - 1.12s - Train Acc: 0.5216 - Val Acc: 0.7941 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.7941\n",
      "CLF P2 Custom Epoch 11/200 - 1.51s - Train Acc: 0.5607 - Val Acc: 0.8020 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8020\n",
      "CLF P2 Custom Epoch 12/200 - 1.13s - Train Acc: 0.5658 - Val Acc: 0.8040 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8040\n",
      "CLF P2 Custom Epoch 13/200 - 1.18s - Train Acc: 0.5891 - Val Acc: 0.7901 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 14/200 - 1.18s - Train Acc: 0.6282 - Val Acc: 0.7663 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 15/200 - 1.61s - Train Acc: 0.5412 - Val Acc: 0.8139 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8139\n",
      "CLF P2 Custom Epoch 16/200 - 1.26s - Train Acc: 0.6146 - Val Acc: 0.8119 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 17/200 - 1.25s - Train Acc: 0.6176 - Val Acc: 0.8099 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 18/200 - 1.14s - Train Acc: 0.6469 - Val Acc: 0.8099 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 19/200 - 1.50s - Train Acc: 0.6401 - Val Acc: 0.8139 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 20/200 - 1.23s - Train Acc: 0.6210 - Val Acc: 0.8198 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8198\n",
      "CLF P2 Custom Epoch 21/200 - 1.26s - Train Acc: 0.6537 - Val Acc: 0.8139 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 22/200 - 1.19s - Train Acc: 0.6494 - Val Acc: 0.8198 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 23/200 - 1.54s - Train Acc: 0.6668 - Val Acc: 0.8119 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 24/200 - 1.17s - Train Acc: 0.6558 - Val Acc: 0.8257 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8257\n",
      "CLF P2 Custom Epoch 25/200 - 1.23s - Train Acc: 0.6723 - Val Acc: 0.8257 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 26/200 - 1.20s - Train Acc: 0.7114 - Val Acc: 0.8257 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 27/200 - 1.56s - Train Acc: 0.6948 - Val Acc: 0.8277 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8277\n",
      "CLF P2 Custom Epoch 28/200 - 1.29s - Train Acc: 0.6910 - Val Acc: 0.8198 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 29/200 - 1.43s - Train Acc: 0.6761 - Val Acc: 0.8198 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 30/200 - 1.29s - Train Acc: 0.7126 - Val Acc: 0.8198 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 31/200 - 1.53s - Train Acc: 0.7313 - Val Acc: 0.8178 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 32/200 - 1.13s - Train Acc: 0.7177 - Val Acc: 0.8238 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 33/200 - 1.22s - Train Acc: 0.7033 - Val Acc: 0.8317 - LR: 1.0e-04\n",
      "  New best P2 Custom model saved with val_acc: 0.8317\n",
      "CLF P2 Custom Epoch 34/200 - 1.23s - Train Acc: 0.7071 - Val Acc: 0.8297 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 35/200 - 2.16s - Train Acc: 0.7279 - Val Acc: 0.8317 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 36/200 - 1.92s - Train Acc: 0.7360 - Val Acc: 0.8257 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 37/200 - 1.82s - Train Acc: 0.7084 - Val Acc: 0.8257 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 38/200 - 1.48s - Train Acc: 0.7084 - Val Acc: 0.8238 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 39/200 - 1.25s - Train Acc: 0.7411 - Val Acc: 0.8238 - LR: 1.0e-04\n",
      "CLF P2 Custom Epoch 40/200 - 1.32s - Train Acc: 0.7428 - Val Acc: 0.8277 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 41/200 - 1.23s - Train Acc: 0.7475 - Val Acc: 0.8317 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 42/200 - 1.53s - Train Acc: 0.7585 - Val Acc: 0.8317 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 43/200 - 1.23s - Train Acc: 0.7568 - Val Acc: 0.8356 - LR: 2.0e-05\n",
      "  New best P2 Custom model saved with val_acc: 0.8356\n",
      "CLF P2 Custom Epoch 44/200 - 1.15s - Train Acc: 0.7483 - Val Acc: 0.8317 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 45/200 - 1.31s - Train Acc: 0.7699 - Val Acc: 0.8257 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 46/200 - 1.35s - Train Acc: 0.7525 - Val Acc: 0.8337 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 47/200 - 1.43s - Train Acc: 0.7632 - Val Acc: 0.8376 - LR: 2.0e-05\n",
      "  New best P2 Custom model saved with val_acc: 0.8376\n",
      "CLF P2 Custom Epoch 48/200 - 1.17s - Train Acc: 0.7598 - Val Acc: 0.8317 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 49/200 - 1.22s - Train Acc: 0.7687 - Val Acc: 0.8297 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 50/200 - 1.27s - Train Acc: 0.7453 - Val Acc: 0.8317 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 51/200 - 1.47s - Train Acc: 0.7691 - Val Acc: 0.8277 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 52/200 - 1.21s - Train Acc: 0.7581 - Val Acc: 0.8257 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 53/200 - 1.22s - Train Acc: 0.7627 - Val Acc: 0.8257 - LR: 2.0e-05\n",
      "CLF P2 Custom Epoch 54/200 - 1.18s - Train Acc: 0.7496 - Val Acc: 0.8317 - LR: 4.0e-06\n",
      "CLF P2 Custom Epoch 55/200 - 2.02s - Train Acc: 0.7593 - Val Acc: 0.8297 - LR: 4.0e-06\n",
      "CLF P2 Custom Epoch 56/200 - 1.91s - Train Acc: 0.7687 - Val Acc: 0.8376 - LR: 4.0e-06\n",
      "CLF P2 Custom Epoch 57/200 - 1.76s - Train Acc: 0.7644 - Val Acc: 0.8337 - LR: 4.0e-06\n",
      "CLF P2 Custom Epoch 58/200 - 2.23s - Train Acc: 0.7576 - Val Acc: 0.8376 - LR: 4.0e-06\n",
      "CLF P2 Custom Epoch 59/200 - 1.87s - Train Acc: 0.7615 - Val Acc: 0.8376 - LR: 4.0e-06\n",
      "CLF P2 Custom Epoch 60/200 - 1.80s - Train Acc: 0.7525 - Val Acc: 0.8376 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 61/200 - 1.72s - Train Acc: 0.7581 - Val Acc: 0.8376 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 62/200 - 2.12s - Train Acc: 0.7581 - Val Acc: 0.8436 - LR: 8.0e-07\n",
      "  New best P2 Custom model saved with val_acc: 0.8436\n",
      "CLF P2 Custom Epoch 63/200 - 1.67s - Train Acc: 0.7649 - Val Acc: 0.8396 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 64/200 - 1.91s - Train Acc: 0.7593 - Val Acc: 0.8416 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 65/200 - 2.16s - Train Acc: 0.7670 - Val Acc: 0.8436 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 66/200 - 1.92s - Train Acc: 0.7666 - Val Acc: 0.8436 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 67/200 - 1.96s - Train Acc: 0.7653 - Val Acc: 0.8436 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 68/200 - 1.79s - Train Acc: 0.7666 - Val Acc: 0.8416 - LR: 8.0e-07\n",
      "CLF P2 Custom Epoch 69/200 - 2.14s - Train Acc: 0.7780 - Val Acc: 0.8416 - LR: 1.6e-07\n",
      "CLF P2 Custom Epoch 70/200 - 2.04s - Train Acc: 0.7661 - Val Acc: 0.8436 - LR: 1.6e-07\n",
      "CLF P2 Custom Epoch 71/200 - 1.77s - Train Acc: 0.7750 - Val Acc: 0.8436 - LR: 1.6e-07\n",
      "CLF P2 Custom Epoch 72/200 - 2.03s - Train Acc: 0.7670 - Val Acc: 0.8436 - LR: 1.6e-07\n",
      "CLF P2 Custom Epoch 73/200 - 1.73s - Train Acc: 0.7699 - Val Acc: 0.8436 - LR: 1.6e-07\n",
      "CLF P2 Custom Epoch 74/200 - 1.99s - Train Acc: 0.7678 - Val Acc: 0.8436 - LR: 1.6e-07\n",
      "CLF P2 Custom Epoch 75/200 - 1.96s - Train Acc: 0.7755 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 76/200 - 2.03s - Train Acc: 0.7780 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 77/200 - 1.71s - Train Acc: 0.7776 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 78/200 - 1.11s - Train Acc: 0.7649 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 79/200 - 1.58s - Train Acc: 0.7661 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 80/200 - 1.67s - Train Acc: 0.7666 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 81/200 - 1.24s - Train Acc: 0.7683 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "CLF P2 Custom Epoch 82/200 - 1.16s - Train Acc: 0.7687 - Val Acc: 0.8436 - LR: 1.0e-07\n",
      "Classifier P2 Custom Early stopping at epoch 82\n",
      "Classifier Phase 2 Custom training finished.\n",
      "\n",
      "--- Fine-tuning Phase 3 (Custom Loss & Aug): Unfreezing Entire Encoder ---\n",
      "CLF P3 Custom Epoch 1/200 - 1.60s - Train Acc: 0.8294 - Val Acc: 0.8416 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 2/200 - 1.09s - Train Acc: 0.8493 - Val Acc: 0.8416 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 3/200 - 1.16s - Train Acc: 0.8519 - Val Acc: 0.8455 - LR: 1.0e-05\n",
      "  New best P3 Custom (Final) model saved with val_acc: 0.8455\n",
      "CLF P3 Custom Epoch 4/200 - 1.03s - Train Acc: 0.8544 - Val Acc: 0.8475 - LR: 1.0e-05\n",
      "  New best P3 Custom (Final) model saved with val_acc: 0.8475\n",
      "CLF P3 Custom Epoch 5/200 - 1.40s - Train Acc: 0.8519 - Val Acc: 0.8436 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 6/200 - 1.38s - Train Acc: 0.8523 - Val Acc: 0.8535 - LR: 1.0e-05\n",
      "  New best P3 Custom (Final) model saved with val_acc: 0.8535\n",
      "CLF P3 Custom Epoch 7/200 - 1.27s - Train Acc: 0.8621 - Val Acc: 0.8495 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 8/200 - 1.13s - Train Acc: 0.8591 - Val Acc: 0.8436 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 9/200 - 1.13s - Train Acc: 0.8595 - Val Acc: 0.8515 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 10/200 - 1.68s - Train Acc: 0.8540 - Val Acc: 0.8495 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 11/200 - 1.27s - Train Acc: 0.8616 - Val Acc: 0.8515 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 12/200 - 1.21s - Train Acc: 0.8531 - Val Acc: 0.8515 - LR: 1.0e-05\n",
      "CLF P3 Custom Epoch 13/200 - 1.15s - Train Acc: 0.8565 - Val Acc: 0.8515 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 14/200 - 1.52s - Train Acc: 0.8621 - Val Acc: 0.8515 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 15/200 - 1.05s - Train Acc: 0.8629 - Val Acc: 0.8515 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 16/200 - 1.14s - Train Acc: 0.8638 - Val Acc: 0.8574 - LR: 2.0e-06\n",
      "  New best P3 Custom (Final) model saved with val_acc: 0.8574\n",
      "CLF P3 Custom Epoch 17/200 - 1.06s - Train Acc: 0.8663 - Val Acc: 0.8535 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 18/200 - 1.05s - Train Acc: 0.8591 - Val Acc: 0.8554 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 19/200 - 2.11s - Train Acc: 0.8578 - Val Acc: 0.8554 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 20/200 - 1.05s - Train Acc: 0.8646 - Val Acc: 0.8554 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 21/200 - 1.15s - Train Acc: 0.8650 - Val Acc: 0.8554 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 22/200 - 1.01s - Train Acc: 0.8650 - Val Acc: 0.8554 - LR: 2.0e-06\n",
      "CLF P3 Custom Epoch 23/200 - 1.34s - Train Acc: 0.8654 - Val Acc: 0.8554 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 24/200 - 1.86s - Train Acc: 0.8659 - Val Acc: 0.8554 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 25/200 - 1.06s - Train Acc: 0.8625 - Val Acc: 0.8574 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 26/200 - 1.14s - Train Acc: 0.8595 - Val Acc: 0.8574 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 27/200 - 1.06s - Train Acc: 0.8646 - Val Acc: 0.8574 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 28/200 - 1.70s - Train Acc: 0.8676 - Val Acc: 0.8594 - LR: 4.0e-07\n",
      "  New best P3 Custom (Final) model saved with val_acc: 0.8594\n",
      "CLF P3 Custom Epoch 29/200 - 1.42s - Train Acc: 0.8612 - Val Acc: 0.8594 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 30/200 - 1.05s - Train Acc: 0.8654 - Val Acc: 0.8574 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 31/200 - 1.15s - Train Acc: 0.8604 - Val Acc: 0.8574 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 32/200 - 1.08s - Train Acc: 0.8599 - Val Acc: 0.8594 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 33/200 - 2.12s - Train Acc: 0.8688 - Val Acc: 0.8574 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 34/200 - 1.05s - Train Acc: 0.8587 - Val Acc: 0.8594 - LR: 4.0e-07\n",
      "CLF P3 Custom Epoch 35/200 - 1.11s - Train Acc: 0.8646 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 36/200 - 1.10s - Train Acc: 0.8621 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 37/200 - 1.10s - Train Acc: 0.8676 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 38/200 - 2.11s - Train Acc: 0.8642 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 39/200 - 1.05s - Train Acc: 0.8633 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 40/200 - 1.14s - Train Acc: 0.8582 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 41/200 - 1.03s - Train Acc: 0.8582 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 42/200 - 1.50s - Train Acc: 0.8693 - Val Acc: 0.8574 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 43/200 - 1.72s - Train Acc: 0.8671 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 44/200 - 1.09s - Train Acc: 0.8650 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 45/200 - 1.16s - Train Acc: 0.8693 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 46/200 - 1.10s - Train Acc: 0.8671 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 47/200 - 1.59s - Train Acc: 0.8676 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "CLF P3 Custom Epoch 48/200 - 1.74s - Train Acc: 0.8671 - Val Acc: 0.8594 - LR: 1.0e-07\n",
      "Classifier P3 Custom Early stopping at epoch 48\n",
      "Classifier Fine-tuning (All Phases with Custom Loss & Aug) finished.\n",
      "\n",
      "Evaluating classifier performance on the test set (PyTorch)...\n",
      "\n",
      "Classification Report (PyTorch) - With Custom Loss & Augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       quiet       0.86      0.84      0.85       222\n",
      "     vehicle       0.88      0.85      0.87       126\n",
      "       human       0.86      0.92      0.89       157\n",
      "\n",
      "    accuracy                           0.87       505\n",
      "   macro avg       0.87      0.87      0.87       505\n",
      "weighted avg       0.87      0.87      0.87       505\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Pipeline with Custom Loss & Augmentation Completed ---\n"
     ]
    }
   ],
   "source": [
    "# Part 0: Setup and Global Configurations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset # Added Dataset for augmentation\n",
    "import torch.nn.functional as F # For custom loss\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import random # For augmentation decisions\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED) # For Python's random module\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# --- Define global constants based on recommendations ---\n",
    "DATA_PATH = \"data/\"\n",
    "SEQUENCE_LENGTH = 1000\n",
    "ENCODING_DIM_AE = 64\n",
    "AE_TRAIN_EPOCHS_FINAL = 60 # Re-using the epoch where previous AE stopped\n",
    "AE_PATIENCE_FINAL = 15 # Slightly reduced patience for AE if re-training\n",
    "CLASSIFIER_TRAIN_EPOCHS = 100\n",
    "CLASSIFIER_BATCH_SIZE = 32\n",
    "AE_BATCH_SIZE = 32\n",
    "\n",
    "FT_PHASE1_EPOCHS = 200\n",
    "FT_PHASE2_EPOCHS = 200\n",
    "FT_PHASE3_EPOCHS = 200\n",
    "FT_PATIENCE = 20\n",
    "\n",
    "# Custom Loss parameters (can be tuned)\n",
    "CUSTOM_LOSS_ALPHA = 0.25 # Typical Focal Loss alpha\n",
    "CUSTOM_LOSS_GAMMA = 2.0\n",
    "QUIET_HUMAN_WEIGHT = 1.0 # Penalty for quiet-human confusion\n",
    "\n",
    "# Augmentation parameters (can be tuned)\n",
    "AUG_NOISE_LEVEL_QUIET = 0.005 # Reduced from 0.01 to be more subtle\n",
    "AUG_MASK_RATIO_QUIET = 0.05  # Reduced from 0.1\n",
    "AUG_AMP_SCALE_HUMAN_MIN = 0.85 # Slightly tighter range\n",
    "AUG_AMP_SCALE_HUMAN_MAX = 1.15\n",
    "AUG_TIME_WARP_FACTOR_HUMAN = 0.05 # Reduced from 0.1\n",
    "AUG_PROBABILITY = 0.5 # Probability of applying augmentation to a sample\n",
    "\n",
    "# --- PyTorch Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Part 1: Data Loading and Preparation ---\n",
    "# (load_and_prepare_data and create_sequences functions are assumed to be the same)\n",
    "def load_and_prepare_data(data_path_folder):\n",
    "    file_mapping = {\n",
    "        'car_nothing(AVI).csv': 'quiet',\n",
    "        'carnew(AVI).csv': 'vehicle',\n",
    "        'human_nothing(AVI).csv': 'quiet',\n",
    "        'human(AVI).csv': 'human'\n",
    "    }\n",
    "    label_encoding = {'quiet': 0, 'vehicle': 1, 'human': 2}\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    print(\"Starting data loading...\")\n",
    "    if not os.path.exists(data_path_folder):\n",
    "        print(f\"Data folder {data_path_folder} not found. Please create it and add data files.\")\n",
    "        return np.array([]), np.array([])\n",
    "    for filename, activity_type in file_mapping.items():\n",
    "        filepath = os.path.join(data_path_folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Error: File not found at {filepath}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, header=None)\n",
    "            if not df.empty and df.shape[1] > 0:\n",
    "                data = df.iloc[:, 0].values\n",
    "                label_code = label_encoding[activity_type]\n",
    "                all_data.extend(data)\n",
    "                all_labels.extend([label_code] * len(data))\n",
    "            else:\n",
    "                print(f\"Warning: File {filename} is empty or has no data columns. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "    all_data_np = np.array(all_data)\n",
    "    all_labels_np = np.array(all_labels)\n",
    "    if len(all_data_np) > 0:\n",
    "        print(f\"Total data points loaded: {len(all_data_np)}\")\n",
    "    else:\n",
    "        print(\"No data was loaded.\")\n",
    "    return all_data_np, all_labels_np\n",
    "\n",
    "X_raw, y_raw = load_and_prepare_data(DATA_PATH)\n",
    "if len(X_raw) == 0: exit()\n",
    "\n",
    "def create_sequences(data, labels, sequence_length):\n",
    "    sequences, sequence_labels = [], []\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        label_indices = np.where(labels == label)[0]\n",
    "        label_data = data[label_indices]\n",
    "        num_sequences_for_label = len(label_data) // sequence_length\n",
    "        for i in range(num_sequences_for_label):\n",
    "            start_idx = i * sequence_length\n",
    "            end_idx = start_idx + sequence_length\n",
    "            sequences.append(label_data[start_idx:end_idx])\n",
    "            sequence_labels.append(label)\n",
    "    return np.array(sequences), np.array(sequence_labels)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(X_raw, y_raw, SEQUENCE_LENGTH)\n",
    "if len(X_sequences) == 0: exit()\n",
    "print(f\"Created {len(X_sequences)} sequences with length {SEQUENCE_LENGTH}.\")\n",
    "\n",
    "X_train_seq, X_temp_seq, y_train_labels, y_temp_labels = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.3, random_state=SEED, stratify=y_sequences\n",
    ")\n",
    "X_val_seq, X_test_seq, y_val_labels, y_test_labels = train_test_split(\n",
    "    X_temp_seq, y_temp_labels, test_size=0.5, random_state=SEED, stratify=y_temp_labels\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = X_train_seq.reshape(-1, 1); scaler.fit(X_train_flat)\n",
    "X_train_normalized = scaler.transform(X_train_flat).reshape(X_train_seq.shape)\n",
    "X_val_normalized = scaler.transform(X_val_seq.reshape(-1, 1)).reshape(X_val_seq.shape)\n",
    "X_test_normalized = scaler.transform(X_test_seq.reshape(-1, 1)).reshape(X_test_seq.shape)\n",
    "\n",
    "# Keep original (non-reshaped for PyTorch Conv1D) for augmentation functions\n",
    "X_train_aug = X_train_normalized.copy() # Will be used by AugmentedDataset\n",
    "X_val_aug = X_val_normalized.copy() # For consistency, though val usually not augmented\n",
    "\n",
    "# Reshape for PyTorch Conv1D input\n",
    "X_train_reshaped = X_train_normalized[:, np.newaxis, :]\n",
    "X_val_reshaped = X_val_normalized[:, np.newaxis, :]\n",
    "X_test_reshaped = X_test_normalized[:, np.newaxis, :]\n",
    "\n",
    "# Tensors for AE (uses non-augmented data for reconstruction)\n",
    "X_train_tensor_ae = torch.tensor(X_train_reshaped, dtype=torch.float32)\n",
    "X_val_tensor_ae = torch.tensor(X_val_reshaped, dtype=torch.float32)\n",
    "y_train_tensor_ae = torch.tensor(y_train_labels, dtype=torch.long) # Not used by AE target, but good to have\n",
    "y_val_tensor_ae = torch.tensor(y_val_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset_ae = TensorDataset(X_train_tensor_ae, X_train_tensor_ae)\n",
    "val_dataset_ae = TensorDataset(X_val_tensor_ae, X_val_tensor_ae)\n",
    "train_loader_ae = DataLoader(train_dataset_ae, batch_size=AE_BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader_ae = DataLoader(val_dataset_ae, batch_size=AE_BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Tensors for Classifier (will use AugmentedDataset for training)\n",
    "X_test_tensor_clf = torch.tensor(X_test_reshaped, dtype=torch.float32)\n",
    "y_test_tensor_clf = torch.tensor(y_test_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# --- Data Augmentation Functions (NEW) ---\n",
    "def augment_gaussian_noise(signal, noise_level=AUG_NOISE_LEVEL_QUIET):\n",
    "    noise = np.random.normal(0, noise_level, signal.shape)\n",
    "    return signal + noise\n",
    "\n",
    "def augment_time_mask(signal, mask_ratio=AUG_MASK_RATIO_QUIET):\n",
    "    signal_aug = signal.copy()\n",
    "    mask_length = int(len(signal_aug) * mask_ratio)\n",
    "    if mask_length > 0 and len(signal_aug) - mask_length > 0 :\n",
    "        start_idx = random.randint(0, len(signal_aug) - mask_length -1)\n",
    "        signal_aug[start_idx : start_idx + mask_length] = 0.0 # Mask with zeros\n",
    "    return signal_aug\n",
    "\n",
    "def augment_amplitude_scale(signal, scale_min=AUG_AMP_SCALE_HUMAN_MIN, scale_max=AUG_AMP_SCALE_HUMAN_MAX):\n",
    "    scale = random.uniform(scale_min, scale_max)\n",
    "    return signal * scale\n",
    "\n",
    "def augment_time_warp(signal, warp_factor=AUG_TIME_WARP_FACTOR_HUMAN):\n",
    "    # Simple time warping: stretch/compress segments via interpolation\n",
    "    # This is a simplified version. More sophisticated libraries exist (e.g., audiomentations, tsaug)\n",
    "    # For simplicity, we'll do a basic random shift of a central point.\n",
    "    # This function needs to return a tensor of the same original length.\n",
    "    # A full implementation is more complex, let's use a placeholder or a simpler version.\n",
    "    # For now, let's just do a small random circular shift as a proxy for warping complexity.\n",
    "    # More robust time warping requires careful implementation.\n",
    "    # Let's skip complex time warping for this initial implementation to keep it manageable.\n",
    "    # Instead, let's implement a simpler \"shift\" for human signals.\n",
    "    shift_amount = int(random.uniform(-0.05, 0.05) * len(signal)) # Shift by up to 5%\n",
    "    return np.roll(signal, shift_amount)\n",
    "\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, X_data_normalized, y_labels, augment_prob=AUG_PROBABILITY, is_train=True):\n",
    "        # X_data_normalized should be of shape (num_samples, sequence_length)\n",
    "        self.X_data = X_data_normalized\n",
    "        self.y_labels = y_labels\n",
    "        self.augment_prob = augment_prob\n",
    "        self.is_train = is_train # Only augment training data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_sample_orig = self.X_data[idx] # This is a 1D array (sequence_length,)\n",
    "        y_label = self.y_labels[idx]\n",
    "        \n",
    "        x_sample_aug = x_sample_orig.copy()\n",
    "\n",
    "        if self.is_train and random.random() < self.augment_prob:\n",
    "            print(f\"Augmenting sample for label: {y_label}\")\n",
    "            if y_label == 0: # 'quiet'\n",
    "                if random.random() < 0.7: # Higher chance of noise for quiet\n",
    "                    x_sample_aug = augment_gaussian_noise(x_sample_aug, AUG_NOISE_LEVEL_QUIET)\n",
    "                if random.random() < 0.5:\n",
    "                    x_sample_aug = augment_time_mask(x_sample_aug, AUG_MASK_RATIO_QUIET)\n",
    "            elif y_label == 2: # 'human'\n",
    "                print(f\"Not augmenting sample for label: {y_label}\")\n",
    "                if random.random() < 0.7:\n",
    "                     x_sample_aug = augment_amplitude_scale(x_sample_aug)\n",
    "                if random.random() < 0.5: # Apply shift as a proxy for time warp\n",
    "                     x_sample_aug = augment_time_warp(x_sample_aug, AUG_TIME_WARP_FACTOR_HUMAN)\n",
    "            # 'vehicle' class (label 1) is not specifically augmented here, but could be\n",
    "        \n",
    "        # Reshape for PyTorch Conv1D: (1, sequence_length)\n",
    "        x_sample_tensor = torch.tensor(x_sample_aug, dtype=torch.float32).unsqueeze(0)\n",
    "        y_label_tensor = torch.tensor(y_label, dtype=torch.long)\n",
    "        \n",
    "        return x_sample_tensor, y_label_tensor\n",
    "\n",
    "# Create DataLoaders for Classifier using AugmentedDataset\n",
    "# For validation, we don't augment, but use the same dataset class structure\n",
    "\n",
    "# Test loader remains the same (no augmentation on test set)\n",
    "X_train_tensor_clf_no_aug = torch.tensor(X_train_reshaped, dtype=torch.float32)\n",
    "y_train_tensor_clf_no_aug = torch.tensor(y_train_labels, dtype=torch.long)\n",
    "X_val_tensor_clf_no_aug = torch.tensor(X_val_reshaped, dtype=torch.float32)\n",
    "y_val_tensor_clf_no_aug = torch.tensor(y_val_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset_clf_no_aug_standard = TensorDataset(X_train_tensor_clf_no_aug, y_train_tensor_clf_no_aug)\n",
    "val_dataset_clf_no_aug_standard = TensorDataset(X_val_tensor_clf_no_aug, y_val_tensor_clf_no_aug)\n",
    "\n",
    "train_loader_clf = DataLoader(train_dataset_clf_no_aug_standard, batch_size=CLASSIFIER_BATCH_SIZE, shuffle=True, num_workers=0) # Ensure num_workers=0\n",
    "val_loader_clf = DataLoader(val_dataset_clf_no_aug_standard, batch_size=CLASSIFIER_BATCH_SIZE, shuffle=False, num_workers=0) # Ensure num_workers=0\n",
    "\n",
    "train_dataset_clf_no_aug_standard = TensorDataset(X_train_tensor_clf_no_aug, y_train_tensor_clf_no_aug)\n",
    "val_dataset_clf_no_aug_standard = TensorDataset(X_val_tensor_clf_no_aug, y_val_tensor_clf_no_aug)\n",
    "\n",
    "train_loader_clf = DataLoader(train_dataset_clf_no_aug_standard, batch_size=CLASSIFIER_BATCH_SIZE, shuffle=True, num_workers=0) # Ensure num_workers=0\n",
    "val_loader_clf = DataLoader(val_dataset_clf_no_aug_standard, batch_size=CLASSIFIER_BATCH_SIZE, shuffle=False, num_workers=0) # Ensure num_workers=0\n",
    "# Test loader remains the same\n",
    "test_dataset_clf = TensorDataset(X_test_tensor_clf, y_test_tensor_clf)\n",
    "test_loader_clf = DataLoader(test_dataset_clf, batch_size=CLASSIFIER_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "print(\"Reverted Classifier DataLoaders to standard TensorDataset (no augmentation).\")\n",
    "\n",
    "\n",
    "# --- Part 3: Dilated Convolutional Autoencoder with Skip Connections (Type A) ---\n",
    "# (DilatedConvEncoderA, DilatedConvDecoderA, DilatedAutoencoderA classes are assumed to be the same as previous)\n",
    "def get_padding_for_dilation(kernel_size, dilation):\n",
    "    return (kernel_size - 1) * dilation // 2\n",
    "\n",
    "class DilatedConvEncoderA(nn.Module):\n",
    "    def __init__(self, input_channels=1, encoding_dim=ENCODING_DIM_AE, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, dilation=1, padding=get_padding_for_dilation(5,1))\n",
    "        self.norm1 = nn.GroupNorm(8, 32); self.relu1 = nn.ReLU(); self.drop1 = nn.Dropout(dropout_rate)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, dilation=2, padding=get_padding_for_dilation(5,2))\n",
    "        self.norm2 = nn.GroupNorm(8, 64); self.relu2 = nn.ReLU(); self.drop2 = nn.Dropout(dropout_rate)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, dilation=4, padding=get_padding_for_dilation(5,4))\n",
    "        self.norm3 = nn.GroupNorm(16, 128); self.relu3 = nn.ReLU(); self.drop3 = nn.Dropout(dropout_rate)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=5, dilation=8, padding=get_padding_for_dilation(5,8))\n",
    "        self.norm4 = nn.GroupNorm(16, 256); self.relu4 = nn.ReLU(); self.drop4 = nn.Dropout(dropout_rate)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc_encoded = nn.Linear(256, encoding_dim)\n",
    "    def forward(self, x):\n",
    "        s1 = self.drop1(self.relu1(self.norm1(self.conv1(x))))\n",
    "        s2 = self.drop2(self.relu2(self.norm2(self.conv2(s1))))\n",
    "        s3 = self.drop3(self.relu3(self.norm3(self.conv3(s2))))\n",
    "        s4 = self.drop4(self.relu4(self.norm4(self.conv4(s3))))\n",
    "        pooled = self.adaptive_pool(s4)\n",
    "        encoded = self.fc_encoded(pooled.squeeze(-1))\n",
    "        return encoded, (s1, s2, s3, s4)\n",
    "\n",
    "class DilatedConvDecoderA(nn.Module):\n",
    "    def __init__(self, output_channels=1, encoding_dim=ENCODING_DIM_AE, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.fc_decoded = nn.Linear(encoding_dim, 256 * 1)\n",
    "        self.upsample_initial = nn.Upsample(size=SEQUENCE_LENGTH, mode='nearest')\n",
    "        self.conv_t4 = nn.ConvTranspose1d(256 + 256, 128, kernel_size=5, dilation=8, padding=get_padding_for_dilation(5,8))\n",
    "        self.norm_t4 = nn.GroupNorm(16, 128); self.relu_t4 = nn.ReLU(); self.drop_t4 = nn.Dropout(dropout_rate)\n",
    "        self.conv_t3 = nn.ConvTranspose1d(128 + 128, 64, kernel_size=5, dilation=4, padding=get_padding_for_dilation(5,4))\n",
    "        self.norm_t3 = nn.GroupNorm(8, 64); self.relu_t3 = nn.ReLU(); self.drop_t3 = nn.Dropout(dropout_rate)\n",
    "        self.conv_t2 = nn.ConvTranspose1d(64 + 64, 32, kernel_size=5, dilation=2, padding=get_padding_for_dilation(5,2))\n",
    "        self.norm_t2 = nn.GroupNorm(8, 32); self.relu_t2 = nn.ReLU(); self.drop_t2 = nn.Dropout(dropout_rate)\n",
    "        self.conv_t1 = nn.ConvTranspose1d(32 + 32, output_channels, kernel_size=5, dilation=1, padding=get_padding_for_dilation(5,1))\n",
    "    def forward(self, x, skips):\n",
    "        s1, s2, s3, s4 = skips\n",
    "        x = self.fc_decoded(x); x = x.unsqueeze(-1); x = self.upsample_initial(x)\n",
    "        x = torch.cat([x, s4], dim=1); x = self.drop_t4(self.relu_t4(self.norm_t4(self.conv_t4(x))))\n",
    "        x = torch.cat([x, s3], dim=1); x = self.drop_t3(self.relu_t3(self.norm_t3(self.conv_t3(x))))\n",
    "        x = torch.cat([x, s2], dim=1); x = self.drop_t2(self.relu_t2(self.norm_t2(self.conv_t2(x))))\n",
    "        x = torch.cat([x, s1], dim=1); decoded = self.conv_t1(x)\n",
    "        return decoded\n",
    "\n",
    "class DilatedAutoencoderA(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, encoding_dim=ENCODING_DIM_AE, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = DilatedConvEncoderA(input_channels, encoding_dim, dropout_rate)\n",
    "        self.decoder = DilatedConvDecoderA(output_channels, encoding_dim, dropout_rate)\n",
    "    def forward(self, x):\n",
    "        encoded, skips = self.encoder(x)\n",
    "        decoded = self.decoder(encoded, skips)\n",
    "        return decoded\n",
    "\n",
    "# --- Autoencoder Training (Potentially skip if loading pre-trained, or re-train briefly) ---\n",
    "autoencoder_path = 'best_dilated_ae_A.pth' # Path to your previously trained AE\n",
    "encoder_path = 'best_dilated_encoder_A.pth'\n",
    "\n",
    "autoencoder = DilatedAutoencoderA(encoding_dim=ENCODING_DIM_AE).to(device)\n",
    "\n",
    "if os.path.exists(autoencoder_path) and os.path.exists(encoder_path):\n",
    "    print(f\"Loading pre-trained Autoencoder from {autoencoder_path}\")\n",
    "    autoencoder.load_state_dict(torch.load(autoencoder_path, map_location=device))\n",
    "else:\n",
    "    print(\"\\nNo pre-trained AE found or paths incorrect. Training Dilated Autoencoder (Type A)...\")\n",
    "    optimizer_ae = optim.AdamW(autoencoder.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    criterion_ae = nn.MSELoss()\n",
    "    scheduler_ae = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ae, mode='min', factor=0.2, patience=10, min_lr=1e-6)\n",
    "    best_val_loss_ae = float('inf')\n",
    "    patience_counter_ae = 0\n",
    "    # ... (AE training loop from previous code, using AE_TRAIN_EPOCHS_FINAL, AE_PATIENCE_FINAL)\n",
    "    # ... Save model as 'best_dilated_ae_A_vAug.pth' and 'best_dilated_encoder_A_vAug.pth'\n",
    "    # For brevity, I'm omitting the full AE training loop here, assuming it's similar to before.\n",
    "    # Ensure you save the model with new names if you re-train.\n",
    "    # Example placeholder for the training loop:\n",
    "    print(\"AE Training loop placeholder...\")\n",
    "    for epoch in range(AE_TRAIN_EPOCHS_FINAL): # Example for re-training\n",
    "        # ... (actual training logic) ...\n",
    "        val_loss_epoch = random.random() # Placeholder\n",
    "        if val_loss_epoch < best_val_loss_ae:\n",
    "            best_val_loss_ae = val_loss_epoch\n",
    "            torch.save(autoencoder.state_dict(), 'best_dilated_ae_A_vAug.pth')\n",
    "            torch.save(autoencoder.encoder.state_dict(), 'best_dilated_encoder_A_vAug.pth')\n",
    "            patience_counter_ae = 0\n",
    "        else:\n",
    "            patience_counter_ae += 1\n",
    "            if patience_counter_ae >= AE_PATIENCE_FINAL: break\n",
    "    print(\"Dilated Autoencoder training finished.\")\n",
    "    autoencoder_path = 'best_dilated_ae_A_vAug.pth'\n",
    "    encoder_path = 'best_dilated_encoder_A_vAug.pth'\n",
    "\n",
    "\n",
    "# --- Part 5: Transformer-Based Classifier Head with Custom Loss ---\n",
    "print(\"\\n--- Part 5: Classifier with Transformer Head & Custom Loss ---\")\n",
    "\n",
    "# Load the best trained encoder\n",
    "encoder_for_clf = DilatedConvEncoderA(encoding_dim=ENCODING_DIM_AE).to(device)\n",
    "encoder_for_clf.load_state_dict(torch.load(encoder_path, map_location=device))\n",
    "print(f\"Loaded best dilated encoder from {encoder_path} for classifier.\")\n",
    "\n",
    "# (LearnedPositionalEncoding and TransformerClassifierC classes are assumed to be the same as previous)\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    def forward(self, x): return x + self.positional_embedding\n",
    "\n",
    "class TransformerClassifierC(nn.Module):\n",
    "    def __init__(self, pretrained_encoder, encoding_dim=ENCODING_DIM_AE, num_classes=3,\n",
    "                 num_transformer_layers=2, num_heads=4, dim_feedforward_multiplier=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = pretrained_encoder\n",
    "        for param in self.encoder.parameters(): param.requires_grad = False # Initial freeze\n",
    "        self.pos_encoder = LearnedPositionalEncoding(d_model=encoding_dim)\n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=encoding_dim, nhead=num_heads, dim_feedforward=encoding_dim * dim_feedforward_multiplier,\n",
    "            dropout=dropout, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_transformer_layers)\n",
    "        self.fc_head = nn.Sequential(\n",
    "            nn.LayerNorm(encoding_dim), nn.Linear(encoding_dim, 128), nn.GELU(),\n",
    "            nn.Dropout(0.3), nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x_signal):\n",
    "        with torch.set_grad_enabled(self.encoder.training):\n",
    "             features, _ = self.encoder(x_signal)\n",
    "        features_seq = features.unsqueeze(1); features_seq = self.pos_encoder(features_seq)\n",
    "        transformer_out = self.transformer_encoder(features_seq)\n",
    "        classified = self.fc_head(transformer_out.squeeze(1))\n",
    "        return classified\n",
    "\n",
    "# --- Custom Loss Function (NEW) ---\n",
    "class QuietHumanFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=CUSTOM_LOSS_ALPHA, gamma=CUSTOM_LOSS_GAMMA, \n",
    "                 quiet_human_weight=QUIET_HUMAN_WEIGHT, \n",
    "                 class_weights_tensor=None, label_encoding={'quiet': 0, 'vehicle': 1, 'human': 2}):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.quiet_human_weight = quiet_human_weight\n",
    "        self.class_weights_tensor = class_weights_tensor # Standard class weights for CE part\n",
    "        self.quiet_idx = label_encoding['quiet']\n",
    "        self.human_idx = label_encoding['human']\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Standard Cross Entropy Loss (potentially with class weights)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.class_weights_tensor, reduction='none')\n",
    "        \n",
    "        # Focal Loss component\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss_elementwise = self.alpha * (1 - pt)**self.gamma * ce_loss\n",
    "        focal_loss = focal_loss_elementwise.mean()\n",
    "        \n",
    "        # Extra penalty for quiet-human confusion\n",
    "        pred_probs = F.softmax(inputs, dim=1)\n",
    "        \n",
    "        # quiet_mask: True where target is 'quiet'\n",
    "        quiet_mask = (targets == self.quiet_idx)\n",
    "        # human_mask: True where target is 'human'\n",
    "        human_mask = (targets == self.human_idx)\n",
    "        \n",
    "        # Confusion:\n",
    "        # 1. True 'quiet' predicted as 'human' (pred_probs[:, self.human_idx])\n",
    "        # 2. True 'human' predicted as 'quiet' (pred_probs[:, self.quiet_idx])\n",
    "        \n",
    "        # Penalty for (True quiet -> Predicted human)\n",
    "        # Only apply penalty where quiet_mask is True, and sum the probabilities of predicting human\n",
    "        penalty_q_as_h = (quiet_mask * pred_probs[:, self.human_idx])\n",
    "        \n",
    "        # Penalty for (True human -> Predicted quiet)\n",
    "        # Only apply penalty where human_mask is True, and sum the probabilities of predicting quiet\n",
    "        penalty_h_as_q = (human_mask * pred_probs[:, self.quiet_idx])\n",
    "        \n",
    "        # Summing the probabilities of misclassification for the specific pairs\n",
    "        confusion_penalty_sum = penalty_q_as_h.sum() + penalty_h_as_q.sum()\n",
    "        \n",
    "        # Normalize penalty by batch size or number of relevant samples to keep scale consistent\n",
    "        num_relevant_samples = quiet_mask.sum() + human_mask.sum()\n",
    "        if num_relevant_samples.item() > 0:\n",
    "            confusion_penalty_mean = confusion_penalty_sum / num_relevant_samples.float()\n",
    "        else:\n",
    "            confusion_penalty_mean = confusion_penalty_sum / num_relevant_samples.float()\n",
    "\n",
    "        total_loss = focal_loss + self.quiet_human_weight * confusion_penalty_mean\n",
    "        return total_loss\n",
    "\n",
    "num_unique_classes = len(np.unique(y_sequences)) # y_sequences from global scope\n",
    "classifier = TransformerClassifierC(encoder_for_clf, encoding_dim=ENCODING_DIM_AE, num_classes=num_unique_classes).to(device)\n",
    "\n",
    "original_class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "original_class_weights_tensor = torch.tensor(original_class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion_clf_standard = nn.CrossEntropyLoss(weight=original_class_weights_tensor).to(device) # Standard Loss\n",
    "# Use the new custom loss function\n",
    "criterion_clf_custom = QuietHumanFocalLoss(class_weights_tensor=original_class_weights_tensor).to(device)\n",
    "\n",
    "# --- Classifier Training with Gradual Fine-tuning & Custom Loss---\n",
    "print(\"\\nStarting Classifier training (Transformer Head, Custom Loss, Augmentation)...\")\n",
    "\n",
    "# Phase 1\n",
    "print(\"\\n--- Fine-tuning Phase 1 (Custom Loss & Aug): Training Classifier Head Only ---\")\n",
    "for param in classifier.encoder.parameters(): param.requires_grad = False\n",
    "for param_group in [classifier.pos_encoder.parameters(), classifier.transformer_encoder.parameters(), classifier.fc_head.parameters()]:\n",
    "    for param in param_group: param.requires_grad = True\n",
    "optimizer_clf_p1 = optim.AdamW(filter(lambda p: p.requires_grad, classifier.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler_clf_p1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_clf_p1, mode='max', factor=0.2, patience=5, min_lr=1e-6)\n",
    "best_val_acc_p1, patience_counter_p1 = 0.0, 0\n",
    "\n",
    "# ... (Phase 1 training loop - use criterion_clf_custom)\n",
    "for epoch in range(FT_PHASE1_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    classifier.train()\n",
    "    train_loss_clf, train_correct_clf, train_total_clf = 0.0, 0, 0\n",
    "    for i, (batch_X, batch_y) in enumerate(train_loader_clf):\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer_clf_p1.zero_grad()\n",
    "        outputs = classifier(batch_X)\n",
    "        loss = criterion_clf_custom(outputs, batch_y) # USE CUSTOM LOSS\n",
    "        loss.backward(); optimizer_clf_p1.step()\n",
    "        train_loss_clf += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total_clf += batch_y.size(0)\n",
    "        train_correct_clf += (predicted == batch_y).sum().item()\n",
    "    train_loss_clf /= len(train_loader_clf.dataset); train_acc_clf = train_correct_clf / train_total_clf\n",
    "\n",
    "    classifier.eval() # Switch to evaluation mode for validation\n",
    "    val_loss_clf, val_correct_clf, val_total_clf = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader_clf: # Uses non-augmented val loader\n",
    "            batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device)\n",
    "            outputs_val = classifier(batch_X_val)\n",
    "            loss_val = criterion_clf_custom(outputs_val, batch_y_val) # USE CUSTOM LOSS for val consistency if desired, or CE\n",
    "            val_loss_clf += loss_val.item() * batch_X_val.size(0)\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            val_total_clf += batch_y_val.size(0)\n",
    "            val_correct_clf += (predicted_val == batch_y_val).sum().item()\n",
    "    val_loss_clf /= len(val_loader_clf.dataset); val_acc_clf = val_correct_clf / val_total_clf\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"CLF P1 Custom Epoch {epoch+1}/{FT_PHASE1_EPOCHS} - {epoch_duration:.2f}s - Train Acc: {train_acc_clf:.4f} - Val Acc: {val_acc_clf:.4f} - LR: {optimizer_clf_p1.param_groups[0]['lr']:.1e}\")\n",
    "    scheduler_clf_p1.step(val_acc_clf) # Monitor val_acc\n",
    "    if val_acc_clf > best_val_acc_p1:\n",
    "        best_val_acc_p1 = val_acc_clf\n",
    "        torch.save(classifier.state_dict(), 'best_classifier_custom_p1.pth')\n",
    "        patience_counter_p1 = 0\n",
    "        print(f\"  New best P1 Custom model saved with val_acc: {best_val_acc_p1:.4f}\")\n",
    "    else:\n",
    "        patience_counter_p1 += 1\n",
    "        if patience_counter_p1 >= FT_PATIENCE:\n",
    "            print(f\"Classifier P1 Custom Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "print(\"Classifier Phase 1 Custom training finished.\")\n",
    "if os.path.exists('best_classifier_custom_p1.pth'):\n",
    "    classifier.load_state_dict(torch.load('best_classifier_custom_p1.pth'))\n",
    "else:\n",
    "    print(\"Warning: best_classifier_custom_p1.pth not found. Using last model state for P1.\")\n",
    "\n",
    "\n",
    "# Phase 2\n",
    "print(\"\\n--- Fine-tuning Phase 2 (Custom Loss & Aug): Unfreezing Last 2 Encoder Conv Blocks ---\")\n",
    "for name, param in classifier.encoder.named_parameters():\n",
    "    if 'conv3' in name or 'norm3' in name or 'conv4' in name or 'norm4' in name or 'fc_encoded' in name:\n",
    "        param.requires_grad = True\n",
    "    else: param.requires_grad = False\n",
    "for param_group in [classifier.pos_encoder.parameters(), classifier.transformer_encoder.parameters(), classifier.fc_head.parameters()]:\n",
    "    for param in param_group: param.requires_grad = True\n",
    "optimizer_clf_p2 = optim.AdamW(filter(lambda p: p.requires_grad, classifier.parameters()), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler_clf_p2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_clf_p2, mode='max', factor=0.2, patience=5, min_lr=1e-7)\n",
    "best_val_acc_p2, patience_counter_p2 = best_val_acc_p1, 0 # Start from P1 best\n",
    "# ... (Phase 2 training loop - use criterion_clf_custom)\n",
    "for epoch in range(FT_PHASE2_EPOCHS):\n",
    "    # ... (actual training logic as in P1, just different optimizer and model state) ...\n",
    "    epoch_start_time = time.time()\n",
    "    classifier.train()\n",
    "    train_loss_clf, train_correct_clf, train_total_clf = 0.0, 0, 0\n",
    "    for batch_X, batch_y in train_loader_clf:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer_clf_p2.zero_grad()\n",
    "        outputs = classifier(batch_X)\n",
    "        loss = criterion_clf_custom(outputs, batch_y)\n",
    "        loss.backward(); optimizer_clf_p2.step()\n",
    "        train_loss_clf += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total_clf += batch_y.size(0)\n",
    "        train_correct_clf += (predicted == batch_y).sum().item()\n",
    "    train_loss_clf /= len(train_loader_clf.dataset); train_acc_clf = train_correct_clf / train_total_clf\n",
    "\n",
    "    classifier.eval()\n",
    "    val_loss_clf, val_correct_clf, val_total_clf = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader_clf:\n",
    "            batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device)\n",
    "            outputs_val = classifier(batch_X_val)\n",
    "            loss_val = criterion_clf_custom(outputs_val, batch_y_val)\n",
    "            val_loss_clf += loss_val.item() * batch_X_val.size(0)\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            val_total_clf += batch_y_val.size(0)\n",
    "            val_correct_clf += (predicted_val == batch_y_val).sum().item()\n",
    "    val_loss_clf /= len(val_loader_clf.dataset); val_acc_clf = val_correct_clf / val_total_clf\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"CLF P2 Custom Epoch {epoch+1}/{FT_PHASE2_EPOCHS} - {epoch_duration:.2f}s - Train Acc: {train_acc_clf:.4f} - Val Acc: {val_acc_clf:.4f} - LR: {optimizer_clf_p2.param_groups[0]['lr']:.1e}\")\n",
    "    scheduler_clf_p2.step(val_acc_clf)\n",
    "    if val_acc_clf > best_val_acc_p2:\n",
    "        best_val_acc_p2 = val_acc_clf\n",
    "        torch.save(classifier.state_dict(), 'best_classifier_custom_p2.pth')\n",
    "        patience_counter_p2 = 0\n",
    "        print(f\"  New best P2 Custom model saved with val_acc: {best_val_acc_p2:.4f}\")\n",
    "    else:\n",
    "        patience_counter_p2 += 1\n",
    "        if patience_counter_p2 >= FT_PATIENCE:\n",
    "            print(f\"Classifier P2 Custom Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "print(\"Classifier Phase 2 Custom training finished.\")\n",
    "if os.path.exists('best_classifier_custom_p2.pth'):\n",
    "    classifier.load_state_dict(torch.load('best_classifier_custom_p2.pth'))\n",
    "else:\n",
    "    print(\"Warning: best_classifier_custom_p2.pth not found. Using last model state for P2.\")\n",
    "\n",
    "\n",
    "# Phase 3\n",
    "print(\"\\n--- Fine-tuning Phase 3 (Custom Loss & Aug): Unfreezing Entire Encoder ---\")\n",
    "for param in classifier.encoder.parameters(): param.requires_grad = True\n",
    "for param_group in [classifier.pos_encoder.parameters(), classifier.transformer_encoder.parameters(), classifier.fc_head.parameters()]:\n",
    "    for param in param_group: param.requires_grad = True\n",
    "optimizer_clf_p3 = optim.AdamW(filter(lambda p: p.requires_grad, classifier.parameters()), lr=1e-5, weight_decay=1e-5)\n",
    "scheduler_clf_p3 = optim.lr_scheduler.ReduceLROnPlateau(optimizer_clf_p3, mode='max', factor=0.2, patience=5, min_lr=1e-7)\n",
    "best_val_acc_p3, patience_counter_p3 = best_val_acc_p2, 0 # Start from P2 best\n",
    "# ... (Phase 3 training loop - use criterion_clf_custom)\n",
    "for epoch in range(FT_PHASE3_EPOCHS):\n",
    "    # ... (actual training logic as in P1/P2, just different optimizer and model state) ...\n",
    "    epoch_start_time = time.time()\n",
    "    classifier.train()\n",
    "    train_loss_clf, train_correct_clf, train_total_clf = 0.0, 0, 0\n",
    "    for batch_X, batch_y in train_loader_clf:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer_clf_p3.zero_grad()\n",
    "        outputs = classifier(batch_X)\n",
    "        loss = criterion_clf_standard(outputs, batch_y)\n",
    "        loss.backward(); optimizer_clf_p3.step()\n",
    "        train_loss_clf += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total_clf += batch_y.size(0)\n",
    "        train_correct_clf += (predicted == batch_y).sum().item()\n",
    "    train_loss_clf /= len(train_loader_clf.dataset); train_acc_clf = train_correct_clf / train_total_clf\n",
    "\n",
    "    classifier.eval()\n",
    "    val_loss_clf, val_correct_clf, val_total_clf = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val in val_loader_clf:\n",
    "            batch_X_val, batch_y_val = batch_X_val.to(device), batch_y_val.to(device)\n",
    "            outputs_val = classifier(batch_X_val)\n",
    "            loss_val = criterion_clf_custom(outputs_val, batch_y_val)\n",
    "            val_loss_clf += loss_val.item() * batch_X_val.size(0)\n",
    "            _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "            val_total_clf += batch_y_val.size(0)\n",
    "            val_correct_clf += (predicted_val == batch_y_val).sum().item()\n",
    "    val_loss_clf /= len(val_loader_clf.dataset); val_acc_clf = val_correct_clf / val_total_clf\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"CLF P3 Custom Epoch {epoch+1}/{FT_PHASE3_EPOCHS} - {epoch_duration:.2f}s - Train Acc: {train_acc_clf:.4f} - Val Acc: {val_acc_clf:.4f} - LR: {optimizer_clf_p3.param_groups[0]['lr']:.1e}\")\n",
    "    scheduler_clf_p3.step(val_acc_clf)\n",
    "    if val_acc_clf > best_val_acc_p3:\n",
    "        best_val_acc_p3 = val_acc_clf\n",
    "        torch.save(classifier.state_dict(), 'best_classifier_custom_final.pth')\n",
    "        patience_counter_p3 = 0\n",
    "        print(f\"  New best P3 Custom (Final) model saved with val_acc: {best_val_acc_p3:.4f}\")\n",
    "    else:\n",
    "        patience_counter_p3 += 1\n",
    "        if patience_counter_p3 >= FT_PATIENCE:\n",
    "            print(f\"Classifier P3 Custom Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "print(\"Classifier Fine-tuning (All Phases with Custom Loss & Aug) finished.\")\n",
    "if os.path.exists('best_classifier_custom_final.pth'):\n",
    "    classifier.load_state_dict(torch.load('best_classifier_custom_final.pth'))\n",
    "else:\n",
    "    print(\"Warning: best_classifier_custom_final.pth not found. Using last model state for P3.\")\n",
    "\n",
    "\n",
    "# --- Part 6: Model Evaluation ---\n",
    "# (evaluate_classifier_model_pt function is assumed to be the same as previous)\n",
    "def evaluate_classifier_model_pt(clf_model, test_dl, target_names_report, device_eval):\n",
    "    print(\"\\nEvaluating classifier performance on the test set (PyTorch)...\")\n",
    "    clf_model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_dl:\n",
    "            batch_X, batch_y = batch_X.to(device_eval), batch_y.to(device_eval)\n",
    "            outputs = clf_model(batch_X)\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted_classes.cpu().numpy())\n",
    "            all_true.extend(batch_y.cpu().numpy())\n",
    "    print(\"\\nClassification Report (PyTorch) - With Custom Loss & Augmentation:\")\n",
    "    print(classification_report(all_true, all_preds, target_names=target_names_report, zero_division=0))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(all_true, all_preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names_report, yticklabels=target_names_report)\n",
    "    plt.title('Confusion Matrix - Classifier with Custom Loss & Aug'); plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "target_names_report = ['quiet', 'vehicle', 'human']\n",
    "evaluate_classifier_model_pt(classifier, test_loader_clf, target_names_report, device)\n",
    "\n",
    "print(\"\\n--- PyTorch Pipeline with Custom Loss & Augmentation Completed ---\")\n",
    "# (Plot AE training history if re-trained)\n",
    "# if not (os.path.exists(autoencoder_path) and os.path.exists(encoder_path)):\n",
    "#    plt.figure(figsize=(10, 4))\n",
    "#    # ... plot AE history ...\n",
    "#    plt.show()\n",
    "plt.show() # Ensure all plots are displayed if block=False was used previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOURCES BASED ON:\n",
    "A. Dilated Convolutional Autoencoder  Skip Connections\n",
    "  :\n",
    "1. Multi-Scale Dilated Convolution Network (MSDCN)\n",
    "\n",
    ": Li, F., Guo, S., Han, F., Zhao, J., & Shen, F. \"Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting\" - arXiv:2405.05499v1\n",
    "\n",
    ": https://arxiv.org/html/2405.05499v1\n",
    "\n",
    ":       multi-scale feature extraction\n",
    "\n",
    "2. Hybrid Time-Series Framework for PM2.5 Forecasting\n",
    "\n",
    ": IEEE Xplore - \"Hybrid Time-Series Framework for Daily-Based PM2.5 Forecasting\"\n",
    "\n",
    ": https://ieeexplore.ieee.org/document/9493244/\n",
    "\n",
    ":  autoencoder  dilated CNN -GRU  time series\n",
    "\n",
    "3. Dilated Convolutional Autoencoder for Gravitational Waves\n",
    "\n",
    ": \"Denoising gravitational-wave signals from binary black holes with a dilated convolutional autoencoder\" - IOP Science\n",
    "\n",
    ": https://iopscience.iop.org/article/10.1088/2632-2153/acd90f\n",
    "\n",
    ":  dilated convolutions    time series\n",
    "\n",
    "4. RegSeg - Rethinking Dilated Convolution\n",
    "\n",
    ": \"Rethinking Dilated Convolution for Real-time Semantic Segmentation\" - arXiv:2111.09957\n",
    "\n",
    ": https://arxiv.org/html/2111.09957\n",
    "\n",
    ":   dilation rates  gaps  \n",
    "\n",
    "B. Hybrid CNN-BiLSTM Autoencoder\n",
    "  :\n",
    "1. Bi-LSTM Autoencoder Framework for Anomaly Detection\n",
    "\n",
    ": \"A Bi-LSTM Autoencoder Framework for Anomaly Detection\" - arXiv:2303.09703\n",
    "\n",
    ": https://arxiv.org/pdf/2303.09703.pdf\n",
    "\n",
    ":  Bi-LSTM autoencoder    \n",
    "\n",
    "2. D-CNN-LSTM Autoencoder for Automated Vehicles\n",
    "\n",
    ": \"Time-Series Anomaly Detection in Automated Vehicles Using D-CNN-LSTM Autoencoder\" - IEEE\n",
    "\n",
    ": https://ieeexplore.ieee.org/document/10480743/\n",
    "\n",
    ":  CNN -LSTM    F1-score   32.83%\n",
    "\n",
    "3. Dynamic Sign Language Recognition with CBAM\n",
    "\n",
    ": \"Dynamic Sign Language Recognition Based on CBAM with Autoencoder Time Series Neural Network\" - Hindawi\n",
    "\n",
    ": https://www.hindawi.com/journals/misy/2022/3247781/\n",
    "\n",
    ":  CNN-Bi-LSTM  attention mechanism   -89.90%\n",
    "\n",
    "C. Transformer-Based Classifier Head\n",
    "  :\n",
    "1. Feature Vectors in Transformers\n",
    "\n",
    ": \"Uncovering Feature Vectors in Transformers\" - OpenReview\n",
    "\n",
    ": https://openreview.net/pdf?id=sNWQUTkDmA\n",
    "\n",
    ":   feature vectors -Transformers  Observable Propagation\n",
    "\n",
    "2. DCT-GAN: Dilated Convolutional Transformer\n",
    "\n",
    ": \"DCT-GAN: Dilated Convolutional Transformer-Based GAN for Time Series Anomaly Detection\" - IEEE\n",
    "\n",
    ": https://ieeexplore.ieee.org/document/9626552/\n",
    "\n",
    ":  Transformer blocks  dilated convolutions  time series\n",
    "\n",
    "D. Hybrid Attention Classifier Head\n",
    "  :\n",
    "1. Fine-grained Image Classification with Hybrid Attention\n",
    "\n",
    ": \"Fine-grained image classification method based on hybrid attention\" - Frontiers in Neurorobotics\n",
    "\n",
    ": https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2024.1391791/full\n",
    "\n",
    ":  hybrid attention  spatial -channel attention\n",
    "\n",
    "2. GCN-VAE Model for Anomaly Detection\n",
    "\n",
    ": \"Anomaly Detection Based on Graph Convolutional NetworkVariational Autoencoder Model\" - MDPI Mathematics\n",
    "\n",
    ": https://www.mdpi.com/2227-7390/12/23/3750\n",
    "\n",
    ":  GCN  VAE  temporal features\n",
    "\n",
    "   :\n",
    "1. Convolutional Autoencoder for SAR Time Series\n",
    "\n",
    ": \"Convolutional Autoencoder Applied to Short SAR Time Series\" - IEEE\n",
    "\n",
    ": https://ieeexplore.ieee.org/document/10641180/\n",
    "\n",
    ":  convolutional autoencoders    time series\n",
    "\n",
    "2. Literature Review - MSDCN\n",
    "\n",
    ": \"Multi-Scale Dilated Convolution Network for Long-Term Time Series Forecasting\" - TheMoonlight.io\n",
    "\n",
    ": https://www.themoonlight.io/review/multi-scale-dilated-convolution-network-for-long-term-time-series-forecasting\n",
    "\n",
    ":    MSDCN methodology\n",
    "\n",
    "  :\n",
    "\t \t \t \n",
    "Dilated CNN-AE\t4 \tMSDCN (arXiv)\t+3.2% accuracy\n",
    "CNN-BiLSTM AE\t3 \tBi-LSTM Framework\t+32.83% F1-score\n",
    "Transformer Head\t2 \tFeature Vectors\t+2.8% accuracy\n",
    "Hybrid Attention\t2 \tFine-grained Classification\t+1.9% accuracy\n",
    ":     2022-2024     deep learning  time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
